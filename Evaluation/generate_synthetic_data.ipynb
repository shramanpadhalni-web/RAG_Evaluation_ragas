{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data_from_csv(source_file, content_column, id_column=None, display_logs=False):\n",
    "    \"\"\"\n",
    "    Imports and processes text data from a CSV file, organizing it into a corpus.\n",
    "\n",
    "    Parameters:\n",
    "    - source_file (str): The path to the CSV file to load the text data from.\n",
    "    - content_column (str): The name of the column in the CSV that contains the text content.\n",
    "    - id_column (str, optional): The name of the column that contains the unique IDs for each row. If None, indices will be used.\n",
    "    - display_logs (bool): If True, displays logs about the loading and processing progress.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary representing the corpus, with IDs (or indices) as keys and content as values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Display initial log if logging is enabled\n",
    "    if display_logs:\n",
    "        print(f\"Initiating import from CSV: {source_file}\")\n",
    "\n",
    "    # Attempt to read the CSV file into a DataFrame\n",
    "    try:\n",
    "        dataframe = pd.read_csv(source_file)\n",
    "        if display_logs:\n",
    "            print(f'Successfully imported {len(dataframe)} records from the CSV file.')\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Failed to load data from the CSV file: {e}\")\n",
    "\n",
    "    # Validate that the content column exists\n",
    "    if content_column not in dataframe.columns:\n",
    "        raise ValueError(f\"The specified content column '{content_column}' does not exist in the CSV file.\")\n",
    "\n",
    "    # Use the specified id_column or default to the DataFrame index\n",
    "    if id_column and id_column in dataframe.columns:\n",
    "        ids = dataframe[id_column]\n",
    "    else:\n",
    "        if display_logs and id_column:\n",
    "            print(f\"Specified ID column '{id_column}' not found. Defaulting to DataFrame indices.\")\n",
    "        ids = dataframe.index\n",
    "\n",
    "    # Construct the corpus from the DataFrame\n",
    "    corpus_content = {row_id: row_content for row_id, row_content in zip(ids, dataframe[content_column])}\n",
    "\n",
    "    return corpus_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(os.getcwd(), 'medical_tc_test.csv')\n",
    "test_corpus = import_data_from_csv(source_file=file_path,\n",
    "                                   content_column='medical_abstract',\n",
    "                                   display_logs=True)\n",
    "test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define the base directory path.\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "# Path to the directory where the processed corpus will be stored.\n",
    "processed_dir = os.path.join(base_dir, 'processed')\n",
    "\n",
    "# Ensure the directory for processed corpus exists.\n",
    "if not os.path.exists(processed_dir):\n",
    "    os.makedirs(processed_dir)  # os.makedirs can create intermediate directories if needed.\n",
    "\n",
    "# Path to the JSON file within the processed directory.\n",
    "PROCESSED_CORPUS_FPATH = os.path.join(processed_dir, 'data_bank.json')\n",
    "\n",
    "# Write the `test_corpus` dictionary to the JSON file.\n",
    "with open(PROCESSED_CORPUS_FPATH, 'w') as f:\n",
    "    json.dump(test_corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "\n",
    "from llama_index.llms import openai\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory path.\n",
    "base_dir = os.getcwd()\n",
    "# Path to the directory where the processed corpus will be stored.\n",
    "processed_dir = os.path.join(base_dir, 'processed')\n",
    "\n",
    "TEST_QUERIES_FPATH = os.path.join(processed_dir, 'search_terms.json')\n",
    "TEST_RELEVANT_DOCS_FPATH = os.path.join(processed_dir, 'pertinent_documents.json')\n",
    "TEST_ANSWERS_FPATH = os.path.join(processed_dir, 'responses.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PROCESSED_CORPUS_FPATH, 'r+') as f:\n",
    "    test_corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a corpus of text\n",
    "test_corpus = {k: test_corpus[k] for k in list(test_corpus.keys())}\n",
    "test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import re\n",
    "\n",
    "def fetch_qa_pairs_from_corpus(data_corpus, questions_per_section=2, custom_prompt=None, display_progress=False):\n",
    "    \"\"\"\n",
    "    This function dynamically generates question-answer pairs based on a provided corpus.\n",
    "    Each piece of text from the corpus is used to formulate questions that are then answered,\n",
    "    simulating a quiz or test preparation scenario.\n",
    "    \"\"\"\n",
    "    artificial_intelligence = openai.OpenAI(model='gpt-3.5-turbo')\n",
    "    default_prompt = custom_prompt or \"\"\"\\\n",
    "    Below is the context for generating questions and answers.\n",
    "\n",
    "    ---------------------\n",
    "    {context}\n",
    "    ---------------------\n",
    "\n",
    "    Given the context above without using external information,\n",
    "    develop {questions_per_section} question(s) with their brief answer(s),\n",
    "    suitable for a quiz or examination. Keep answers concise, within 1-50 words. \n",
    "    Ensure the generated content varies and aligns closely with the provided context.\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    question_bank = {}\n",
    "    answer_key = {}\n",
    "    document_references = {}\n",
    "\n",
    "    for doc_id, context in tqdm(data_corpus.items(), disable=not display_progress):\n",
    "        dynamic_prompt = default_prompt.format(context=context, questions_per_section=questions_per_section)\n",
    "        \n",
    "        try:\n",
    "            ai_response = artificial_intelligence.complete(dynamic_prompt)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate response for document ID {doc_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        processed_response = str(ai_response).strip().split(\"\\n\")\n",
    "        paired_qa = zip(processed_response[0::2], processed_response[1::2])\n",
    "\n",
    "        for q, a in paired_qa:\n",
    "            q = re.sub(r\"^\\d+[\\).\\s]\", \"\", q).strip()\n",
    "            if q and a:  # Ensures both question and answer are not empty\n",
    "                unique_id = str(uuid.uuid4())\n",
    "                q = q.replace(\"Question:\", \"\").strip()\n",
    "                question_bank[unique_id] = q\n",
    "                a = a.replace(\"Answer:\", \"\").strip()\n",
    "                answer_key[unique_id] = a\n",
    "                document_references[unique_id] = [doc_id]\n",
    "\n",
    "    return question_bank, answer_key, document_references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries, test_answers, test_relevant_docs = fetch_qa_pairs_from_corpus(\n",
    "    test_corpus,\n",
    "    questions_per_section=1,\n",
    "    display_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEST_QUERIES_FPATH, 'w+') as f:\n",
    "    json.dump(test_queries, f)\n",
    "\n",
    "with open(TEST_ANSWERS_FPATH, 'w+') as f:\n",
    "    json.dump(test_answers, f)\n",
    "\n",
    "with open(TEST_RELEVANT_DOCS_FPATH, 'w+') as f:\n",
    "    json.dump(test_relevant_docs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the JSON file within the processed directory.\n",
    "TEST_DATASET_FPATH = os.path.join(processed_dir, 'data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = {\n",
    "    'queries': test_queries,\n",
    "    'answers': test_answers,\n",
    "    'corpus': test_corpus,\n",
    "    'relevant_docs': test_relevant_docs,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(TEST_DATASET_FPATH):\n",
    "    os.remove(TEST_DATASET_FPATH)\n",
    "with open(TEST_DATASET_FPATH, 'w+') as f:\n",
    "    json.dump(test_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
