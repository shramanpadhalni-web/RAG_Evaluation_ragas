{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Append the directory above \"Evaluation\" (i.e., the parent directory) to the sys.path\n",
    "# This assumes you're running this from within the \"Evaluation\" directory\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from db_retriever_module import ChromadbRetrieverModule\n",
    "\n",
    "# Get the current working directory (where your notebook is)\n",
    "current_dir = Path().resolve()\n",
    "\n",
    "# Go two levels up to reach the base directory ('some/')\n",
    "base_dir = current_dir.parent\n",
    "\n",
    "# Add the base directory to the system path\n",
    "sys.path.append(str(base_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerGenerator(dspy.Signature):\n",
    "    \"\"\"\n",
    "    A class to generate answers to questions based on provided context using DSPy's Signature.\n",
    "    \n",
    "    Attributes:\n",
    "        context (dspy.InputField): Text that may contain relevant facts for generating an answer.\n",
    "        question (dspy.InputField): The question for which an answer is sought.\n",
    "        answer (dspy.OutputField): The generated short factual answer, aimed to be between 1 to 15 words.\n",
    "    \"\"\"\n",
    "    context = dspy.InputField(description=\"May contain relevant facts for answering.\")\n",
    "    question = dspy.InputField(description=\"The question to be answered.\")\n",
    "    answer = dspy.OutputField(description=\"Short factual answer to the question, typically 1-15 words.\")\n",
    "\n",
    "class RetrievalAugmentedGenerator(dspy.Module):\n",
    "    \"\"\"\n",
    "    Implements a Retrieval-Augmented Generation (RAG) model by combining retrieval and generation capabilities.\n",
    "\n",
    "    This model first retrieves relevant passages based on the input question and then generates an answer\n",
    "    by considering both the question and the retrieved context.\n",
    "    \n",
    "    Attributes:\n",
    "        num_passages (int): Number of passages to retrieve for context enrichment.\n",
    "        retrieve (dspy.Retrieve): The retrieval component of the RAG model.\n",
    "        generate_answer (dspy.ChainOfThought): The answer generation component based on retrieved context.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_passages=5):\n",
    "        super().__init__()\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(AnswerGenerator)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        \"\"\"\n",
    "        Retrieves relevant passages based on the question and generates an answer.\n",
    "\n",
    "        Args:\n",
    "            question (str): The input question for which an answer is needed.\n",
    "        \n",
    "        Returns:\n",
    "            dspy.Prediction: The context used for answering and the generated answer.\n",
    "        \"\"\"\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import dspy\n",
    "\n",
    "# Assuming RAG and ChromadbRM classes are defined elsewhere in your code\n",
    "# from your_module import RAG, ChromadbRM\n",
    "\n",
    "def setup():\n",
    "    \"\"\"\n",
    "    Initializes and configures the DSPy library for a Retrieval-Augmented Generation (RAG) setup.\n",
    "    \n",
    "    This involves configuring a language model and a retrieval model with specified settings,\n",
    "    and then initializing the RAG module with these configurations.\n",
    "\n",
    "    Returns:\n",
    "        An instance of the RAG class, ready for use.\n",
    "        \n",
    "    Raises:\n",
    "        EnvironmentError: If the OPENAI_API_KEY environment variable is not set.\n",
    "        Exception: For general exceptions related to DSPy configuration or RAG initialization.\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "        \n",
    "        # Retrieve the OpenAI API key safely\n",
    "        openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not openai_api_key:\n",
    "            raise EnvironmentError(\"OPENAI_API_KEY not set in environment variables.\")\n",
    "        \n",
    "        # Configuration parameters\n",
    "        MODEL_NAME = 'gpt-3.5-turbo'\n",
    "        COLLECTION_NAME = \"test-medical_abstract_collection\"\n",
    "        PERSIST_DIR = \"local_chroma.db\"\n",
    "        LOCAL_EMBED_MODEL = \"sentence-transformers/paraphrase-MiniLM-L6-v2\"\n",
    "        \n",
    "        # Configuring the language model\n",
    "        turbo = dspy.OpenAI(model=MODEL_NAME)\n",
    "        \n",
    "        # Configuring the retrieval model\n",
    "        chroma_rm = ChromadbRetrieverModule(db_collection_name=COLLECTION_NAME, persist_directory=PERSIST_DIR,\n",
    "                               local_embed_model=LOCAL_EMBED_MODEL, api_key=openai_api_key)\n",
    "        \n",
    "        # Apply the configurations\n",
    "        dspy.settings.configure(lm=turbo, rm=chroma_rm)\n",
    "        \n",
    "        # Initialize the RAG module\n",
    "        rag_instance = RetrievalAugmentedGenerator()\n",
    "        \n",
    "        logging.info(\"RAG setup completed successfully.\")\n",
    "        \n",
    "        return rag_instance\n",
    "        \n",
    "    except EnvironmentError as env_err:\n",
    "        logging.error(f\"Environment error during setup: {env_err}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to complete RAG setup: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection Count: 578\n"
     ]
    }
   ],
   "source": [
    "rag = setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You saved it in the validate synthetics data ipynb  - synthetic_dataset.csv\n",
    "import pandas as pd\n",
    "df = pd.read_csv(os.path.join(os.getcwd(),\"processed\",\"synthetic_dataset.csv\"))\n",
    "df = df[['question', 'ground_truths']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What was the effect of oropharyngeal anesthesi...</td>\n",
       "      <td>['Oropharyngeal anesthesia led to an increase ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What was the prognostic value of low neutrophi...</td>\n",
       "      <td>['Low neutrophil function, particularly defect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What was the treatment that resulted in both c...</td>\n",
       "      <td>['']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What was the conclusion regarding the role of ...</td>\n",
       "      <td>['The conclusion was that CNS prophylaxis with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the advantages of using duplex Dopple...</td>\n",
       "      <td>['The advantages include absence of toxicity, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Why is congenital hypertrophy of the retinal p...</td>\n",
       "      <td>['Congenital hypertrophy of the RPE serves as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is the main conclusion drawn from the stu...</td>\n",
       "      <td>['The study suggests an abnormality in an intr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the purpose of the back isometric dyna...</td>\n",
       "      <td>['']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What was the effect of prophylactic peroral ac...</td>\n",
       "      <td>['Prophylactic peroral ACV prevented the devel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What unique myopathic changes were observed in...</td>\n",
       "      <td>['Vacuolar changes with periodic acid-Schiff-p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What was the effect of oropharyngeal anesthesi...   \n",
       "1  What was the prognostic value of low neutrophi...   \n",
       "2  What was the treatment that resulted in both c...   \n",
       "3  What was the conclusion regarding the role of ...   \n",
       "4  What are the advantages of using duplex Dopple...   \n",
       "5  Why is congenital hypertrophy of the retinal p...   \n",
       "6  What is the main conclusion drawn from the stu...   \n",
       "7  What is the purpose of the back isometric dyna...   \n",
       "8  What was the effect of prophylactic peroral ac...   \n",
       "9  What unique myopathic changes were observed in...   \n",
       "\n",
       "                                       ground_truths  \n",
       "0  ['Oropharyngeal anesthesia led to an increase ...  \n",
       "1  ['Low neutrophil function, particularly defect...  \n",
       "2                                               ['']  \n",
       "3  ['The conclusion was that CNS prophylaxis with...  \n",
       "4  ['The advantages include absence of toxicity, ...  \n",
       "5  ['Congenital hypertrophy of the RPE serves as ...  \n",
       "6  ['The study suggests an abnormality in an intr...  \n",
       "7                                               ['']  \n",
       "8  ['Prophylactic peroral ACV prevented the devel...  \n",
       "9  ['Vacuolar changes with periodic acid-Schiff-p...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test\n",
    "train, test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the train and test data\n",
    "train.to_csv(os.path.join(os.getcwd(),\"processed\",\"train_synthetic.csv\"), index=False)\n",
    "test.to_csv(os.path.join(os.getcwd(),\"processed\",\"test_synthetic.csv\"), index=False)\n",
    "\n",
    "# load the train and test data\n",
    "train = pd.read_csv(os.path.join(os.getcwd(),\"processed\",\"train_synthetic.csv\"))\n",
    "test = pd.read_csv(os.path.join(os.getcwd(),\"processed\",\"test_synthetic.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8b313855684af897c8f549a0b0d90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing questions:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm  # Use tqdm.auto to automatically select a suitable interface (notebook, terminal, etc.)\n",
    "\n",
    "# Assuming 'test' is your DataFrame and 'rag' is a function defined elsewhere that takes a question and returns a response object\n",
    "\n",
    "# Create an empty list to store rows\n",
    "eval_results_rows = []\n",
    "\n",
    "# Wrap test.iterrows() with tqdm to display a progress bar\n",
    "for index, row in tqdm(test.iterrows(), total=test.shape[0], desc=\"Processing questions\"):\n",
    "    # Get the question\n",
    "    question = row['question']\n",
    "    \n",
    "    # Response from rag\n",
    "    response = rag(question)\n",
    "    \n",
    "    # Create a dictionary to represent a row\n",
    "    row_dict = {\n",
    "        'question': question, \n",
    "        'contexts': response.context, \n",
    "        'answer': response.answer, \n",
    "        'ground_truths': row['ground_truths']\n",
    "    }\n",
    "    \n",
    "    # Append the row dictionary to the list\n",
    "    eval_results_rows.append(row_dict)\n",
    "\n",
    "# Create the df_eval_results DataFrame from the list of rows\n",
    "df_eval_results = pd.DataFrame(eval_results_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the purpose of the back isometric dyna...</td>\n",
       "      <td>[bretylium tosylate versus lidocaine in experi...</td>\n",
       "      <td>Measure and improve back muscle strength and f...</td>\n",
       "      <td>['']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What was the effect of dietary calcium on bloo...</td>\n",
       "      <td>[laser angioplasty in peripheral vascular dise...</td>\n",
       "      <td>Lowered blood pressure in hypertensive rats wi...</td>\n",
       "      <td>['Rats receiving high dietary calcium showed a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What were the main measurements taken during t...</td>\n",
       "      <td>[ultrasonographic assessment of placental abno...</td>\n",
       "      <td>Intramuscular meperidine, promethazine, and ch...</td>\n",
       "      <td>['The main measurements included serial respir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What was the conclusion regarding the role of ...</td>\n",
       "      <td>[three mixed venous saturation catheters in pa...</td>\n",
       "      <td>The conclusion regarding the role of CNS radio...</td>\n",
       "      <td>['The conclusion was that CNS prophylaxis with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What was the prognostic value of low neutrophi...</td>\n",
       "      <td>[trichothiodystrophy with chronic neutropenia ...</td>\n",
       "      <td>Significant in predicting late pyogenic infect...</td>\n",
       "      <td>['Low neutrophil function, particularly defect...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is the purpose of the back isometric dyna...   \n",
       "1  What was the effect of dietary calcium on bloo...   \n",
       "2  What were the main measurements taken during t...   \n",
       "3  What was the conclusion regarding the role of ...   \n",
       "4  What was the prognostic value of low neutrophi...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [bretylium tosylate versus lidocaine in experi...   \n",
       "1  [laser angioplasty in peripheral vascular dise...   \n",
       "2  [ultrasonographic assessment of placental abno...   \n",
       "3  [three mixed venous saturation catheters in pa...   \n",
       "4  [trichothiodystrophy with chronic neutropenia ...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Measure and improve back muscle strength and f...   \n",
       "1  Lowered blood pressure in hypertensive rats wi...   \n",
       "2  Intramuscular meperidine, promethazine, and ch...   \n",
       "3  The conclusion regarding the role of CNS radio...   \n",
       "4  Significant in predicting late pyogenic infect...   \n",
       "\n",
       "                                       ground_truths  \n",
       "0                                               ['']  \n",
       "1  ['Rats receiving high dietary calcium showed a...  \n",
       "2  ['The main measurements included serial respir...  \n",
       "3  ['The conclusion was that CNS prophylaxis with...  \n",
       "4  ['Low neutrophil function, particularly defect...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# df_eval_results ground_truths to list\n",
    "df_eval_results['ground_truths'] = df_eval_results['ground_truths'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, that we have answers for all the questions, we can evaluate the RAG model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_similarity,\n",
    "    context_relevancy\n",
    ")\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "\n",
    "ds = Dataset.from_pandas(df_eval_results)\n",
    "\n",
    "result = evaluate(\n",
    "    ds,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        answer_similarity,\n",
    "        context_relevancy\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_precision': 0.4233, 'faithfulness': 0.0000, 'answer_relevancy': 0.5440, 'context_recall': 0.8000, 'answer_similarity': 0.8402, 'context_relevancy': 0.0000}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log to Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok lets login to wandb\n",
    "# wandb.login(key=\"your - api - key - here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_wandb_run(dataset, result, project_name=\"medical_abstract-rag-synthetic-data-eval\",\n",
    "                     chunk_size=128, sentence_chunk_overlap=16):\n",
    "    \"\"\"\n",
    "    Initializes a Weights & Biases run to log metrics, parameters, and results for tracking experiments.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: The dataset being evaluated or processed in the run.\n",
    "    - result: A dictionary containing the results to log in the run.\n",
    "    - project_name (str): The name of the Weights & Biases project where the run will be logged.\n",
    "    - chunk_size (int): The size of the chunks to split the dataset into.\n",
    "    - sentence_chunk_overlap (int): The overlap size between consecutive dataset chunks.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Start a new Weights & Biases run\n",
    "        wandb.init(project=project_name, config={\n",
    "        \"number_of_questions\": len(ds),\n",
    "        \"comments\": \"Simple QA RAG model with no teleprompter - chunk overlap size 0\",\n",
    "        \"model\": \"RAG\",\n",
    "        \"dataset\": \"Synthetic\",\n",
    "        \"num_passages\": 5,\n",
    "        \"openai_model\": \"gpt-3.5-turbo\",\n",
    "        \"chroma_collection_name\": \"test-medical_abstract_collection\",\n",
    "        \"chroma_persist_directory\": \"local_chroma.db\",\n",
    "        \"chroma_local_embed_model\": \"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n",
    "\n",
    "    })\n",
    "\n",
    "        # Log the result to the current run\n",
    "        wandb.log(result)\n",
    "\n",
    "        # Finish the current run to ensure all data is synced\n",
    "        wandb.finish()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during the Weights & Biases operation: {e}\")\n",
    "        # Optionally, handle exceptions such as retrying the operation or logging the error to a file\n",
    "\n",
    "# Start and log the wandb run\n",
    "start_wandb_run(ds, result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's compile the RAG using teleprompters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are some alternative dosage regimens for ...</td>\n",
       "      <td>['Some alternative dosage regimens for rt-PA i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the major drawbacks associated with u...</td>\n",
       "      <td>['The major drawbacks of using 125I for therap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What was the effect of oropharyngeal anesthesi...</td>\n",
       "      <td>['Oropharyngeal anesthesia led to an increase ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What was the overall risk of developing second...</td>\n",
       "      <td>['']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the role of neural cell adhesion molec...</td>\n",
       "      <td>['N-CAM is involved in direct cell-cell adhesi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What are the advantages of using duplex Dopple...</td>\n",
       "      <td>['The advantages include absence of toxicity, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What was the effect of prophylactic peroral ac...</td>\n",
       "      <td>['Prophylactic peroral ACV prevented the devel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What was the predominant type of IgA found in ...</td>\n",
       "      <td>[\"Secretory IgA comprised 92%, 81.6%, and 76.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the incidence of maternal varicella in...</td>\n",
       "      <td>['Maternal varicella occurs in fewer than five...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Why is congenital hypertrophy of the retinal p...</td>\n",
       "      <td>['Congenital hypertrophy of the RPE serves as ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What are some alternative dosage regimens for ...   \n",
       "1  What are the major drawbacks associated with u...   \n",
       "2  What was the effect of oropharyngeal anesthesi...   \n",
       "3  What was the overall risk of developing second...   \n",
       "4  What is the role of neural cell adhesion molec...   \n",
       "5  What are the advantages of using duplex Dopple...   \n",
       "6  What was the effect of prophylactic peroral ac...   \n",
       "7  What was the predominant type of IgA found in ...   \n",
       "8  What is the incidence of maternal varicella in...   \n",
       "9  Why is congenital hypertrophy of the retinal p...   \n",
       "\n",
       "                                       ground_truths  \n",
       "0  ['Some alternative dosage regimens for rt-PA i...  \n",
       "1  ['The major drawbacks of using 125I for therap...  \n",
       "2  ['Oropharyngeal anesthesia led to an increase ...  \n",
       "3                                               ['']  \n",
       "4  ['N-CAM is involved in direct cell-cell adhesi...  \n",
       "5  ['The advantages include absence of toxicity, ...  \n",
       "6  ['Prophylactic peroral ACV prevented the devel...  \n",
       "7  [\"Secretory IgA comprised 92%, 81.6%, and 76.7...  \n",
       "8  ['Maternal varicella occurs in fewer than five...  \n",
       "9  ['Congenital hypertrophy of the RPE serves as ...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "trainset = []\n",
    "for i in range(5):\n",
    "    ex = dspy.Example(\n",
    "        question=train['question'].iloc[i],\n",
    "        answer=ast.literal_eval(train['ground_truths'].iloc[i])[0]\n",
    "    )\n",
    "    ex = ex.with_inputs('question')\n",
    "    trainset.append(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Example({'question': 'What are some alternative dosage regimens for rt-PA in patients with myocardial infarction?', 'answer': 'Some alternative dosage regimens for rt-PA in patients with myocardial infarction include bolus, front-loaded, and accelerated infusions.'}) (input_keys={'question'}),\n",
       " Example({'question': 'What are the major drawbacks associated with using 125I for therapy in tumours?', 'answer': 'The major drawbacks of using 125I for therapy in tumours include its long 60-day half-life, leading to radiological and waste disposal problems, and the extreme short range of its radiotoxic effects.'}) (input_keys={'question'}),\n",
       " Example({'question': 'What was the effect of oropharyngeal anesthesia on obstructive sleep apnea in the study subjects?', 'answer': 'Oropharyngeal anesthesia led to an increase in obstructive apneas and hypopneas, as well as a higher frequency of oxyhemoglobin desaturations during sleep.'}) (input_keys={'question'}),\n",
       " Example({'question': \"What was the overall risk of developing secondary leukaemia in patients treated for Hodgkin's disease in the British National Lymphoma Investigation studies?\", 'answer': ''}) (input_keys={'question'}),\n",
       " Example({'question': 'What is the role of neural cell adhesion molecule (N-CAM) in neuroendocrine tissues?', 'answer': 'N-CAM is involved in direct cell-cell adhesion in neuroendocrine tissues, as shown by its expression in most neuroendocrine cells and tumors with secretory granules.'}) (input_keys={'question'})]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 22.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 0 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "# Validation logic: check that the predicted answer is correct.\n",
    "# Also check that the retrieved context does actually contain that answer.\n",
    "def check_answer_and_context_validity(example, pred, trace=None):\n",
    "    answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n",
    "    answer_PM = dspy.evaluate.answer_passage_match(example, pred)\n",
    "    return answer_EM and answer_PM\n",
    "\n",
    "# Set up a basic teleprompter, which will compile our RAG program.\n",
    "teleprompter = BootstrapFewShot(metric=check_answer_and_context_validity)\n",
    "\n",
    "# Compile!\n",
    "compiled_rag = teleprompter.compile(RetrievalAugmentedGenerator(), trainset=trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def compile_evaluation_metrics(data, model_response):\n",
    "    \"\"\"\n",
    "    Compiles evaluation metrics from a dataset using responses from a specified model.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The dataset containing questions and ground truths.\n",
    "        model_response (function): The model function to generate responses based on questions.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing original questions, model contexts, model answers, and ground truths.\n",
    "    \"\"\"\n",
    "    compiled_data = []\n",
    "\n",
    "    for _, entry in data.iterrows():\n",
    "        try:\n",
    "            question_text = entry['question']\n",
    "            model_output = model_response(question_text)\n",
    "            result_entry = {\n",
    "                'question': question_text,\n",
    "                'contexts': model_output.context,\n",
    "                'answer': model_output.answer,\n",
    "                'ground_truths': entry['ground_truths']\n",
    "            }\n",
    "            compiled_data.append(result_entry)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question: {question_text}, Error: {e}\")\n",
    "\n",
    "    evaluation_df = pd.DataFrame(compiled_data)\n",
    "\n",
    "    # Safely convert 'ground_truths' from string representation to list\n",
    "    def safe_literal_eval(value):\n",
    "        try:\n",
    "            return ast.literal_eval(value)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return []  # or return a default value such as ['error_parsing']\n",
    "\n",
    "    evaluation_df['ground_truths'] = evaluation_df['ground_truths'].apply(safe_literal_eval)\n",
    "\n",
    "    return evaluation_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_results = compile_evaluation_metrics(test, compiled_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(df_eval_results)\n",
    "\n",
    "result = evaluate(\n",
    "    ds,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        answer_similarity,\n",
    "        context_relevancy\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
